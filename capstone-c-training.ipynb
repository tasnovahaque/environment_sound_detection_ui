{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11636372,"sourceType":"datasetVersion","datasetId":7301102}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n\nsource_folder = '/kaggle/input/environment-sound/Environment_Sound'\ndestination_folder = '/kaggle/working/dataset'\n\ntry:\n    # Create the destination folder if it doesn't exist\n    os.makedirs(destination_folder, exist_ok=True)\n    print(f\"Destination folder created or already exists: {destination_folder}\")\n\n    # Iterate through the subfolders in the source folder\n    for item_name in os.listdir(source_folder):\n        item_path = os.path.join(source_folder, item_name)\n        destination_item_path = os.path.join(destination_folder, item_name)\n\n        # If it's a directory, copy it recursively\n        if os.path.isdir(item_path):\n            shutil.copytree(item_path, destination_item_path)\n            print(f\"Copied folder: {item_name}\")\n        # If it's a file, copy it\n        elif os.path.isfile(item_path):\n            shutil.copy2(item_path, destination_item_path)  # copy2 preserves metadata\n            print(f\"Copied file: {item_name}\")\n\n    print(\"All folders and files copied successfully!\")\n\nexcept FileNotFoundError:\n    print(f\"Error: Source folder not found at {source_folder}\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:49:08.790805Z","iopub.execute_input":"2025-05-09T02:49:08.791434Z","iopub.status.idle":"2025-05-09T02:50:08.876619Z","shell.execute_reply.started":"2025-05-09T02:49:08.791407Z","shell.execute_reply":"2025-05-09T02:50:08.875855Z"}},"outputs":[{"name":"stdout","text":"Destination folder created or already exists: /kaggle/working/dataset\nCopied folder: Bus\nCopied folder: Park\nCopied folder: Metro\nCopied folder: University\nCopied folder: Shopping_Mall\nCopied folder: Restaurant\nCopied folder: Metro_Station\nAll folders and files copied successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom tqdm import tqdm\nimport random\nimport time\nfrom scipy import signal\nfrom google.colab import drive\nimport glob\n\n# Paths\nchunks_dir = '/kaggle/working/dataset'  # Input directory with 10-second chunks\n\n# Set target files per class\nTARGET_FILES_PER_CLASS = 1500\n\n# Create output directory\nos.makedirs(chunks_dir, exist_ok=True)\n\n# Set random seed for reproducibility\nnp.random.seed(42)\nrandom.seed(42)\n\n# Augmentation functions\ndef add_white_noise(audio, noise_factor=0.005):\n    \"\"\"Add white noise to the audio signal.\"\"\"\n    noise = np.random.randn(len(audio))\n    augmented_audio = audio + noise_factor * noise\n    return augmented_audio\n\ndef time_stretch(audio, rate=1.0):\n    \"\"\"Time stretch the audio signal without changing the pitch.\"\"\"\n    return librosa.effects.time_stretch(audio, rate=rate)\n\ndef pitch_shift(audio, sr, n_steps=2):\n    \"\"\"Shift the pitch of the audio signal.\"\"\"\n    return librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n\ndef change_volume(audio, factor=1.0):\n    \"\"\"Change the volume of the audio signal.\"\"\"\n    return audio * factor\n\ndef add_background_noise(audio, noise_audio, noise_factor=0.2):\n    \"\"\"Mix background noise with audio.\"\"\"\n    # Make sure the noise is the same length as the audio\n    if len(noise_audio) < len(audio):\n        # If noise is shorter, repeat it\n        repeats = int(np.ceil(len(audio) / len(noise_audio)))\n        noise_audio = np.tile(noise_audio, repeats)\n\n    # Trim to match the length of audio\n    noise_audio = noise_audio[:len(audio)]\n\n    # Mix with the given factor\n    augmented_audio = audio + noise_factor * noise_audio\n\n    # Normalize to prevent clipping\n    if np.max(np.abs(augmented_audio)) > 1.0:\n        augmented_audio = augmented_audio / np.max(np.abs(augmented_audio))\n\n    return augmented_audio\n\ndef apply_filter(audio, sr, filter_type='lowpass', cutoff=1000):\n    \"\"\"Apply a filter to the audio signal.\"\"\"\n    nyquist = sr // 2\n    normalized_cutoff = cutoff / nyquist\n\n    if filter_type == 'lowpass':\n        b, a = signal.butter(5, normalized_cutoff, btype='low')\n    elif filter_type == 'highpass':\n        b, a = signal.butter(5, normalized_cutoff, btype='high')\n    elif filter_type == 'bandpass':\n        b, a = signal.butter(5, [normalized_cutoff * 0.5, normalized_cutoff], btype='band')\n    else:\n        return audio\n\n    return signal.filtfilt(b, a, audio)\n\ndef speed_up(audio, factor=1.2):\n    \"\"\"Speed up the audio by resampling.\"\"\"\n    indices = np.round(np.arange(0, len(audio), factor)).astype(int)\n    indices = indices[indices < len(audio)]\n    return audio[indices]\n\ndef slow_down(audio, factor=0.8):\n    \"\"\"Slow down the audio by resampling.\"\"\"\n    return speed_up(audio, 1/factor)\n\ndef time_shift(audio, shift_factor=0.2):\n    \"\"\"Shift the audio in time.\"\"\"\n    shift = int(len(audio) * shift_factor)\n    return np.roll(audio, shift)\n\ndef reverb_effect(audio, sr, decay=2.0):\n    \"\"\"Add a simple reverb effect by convolving with an exponentially decaying filter.\"\"\"\n    reverb_length = int(sr * decay)\n    impulse_response = np.exp(-np.arange(reverb_length) / sr * 3)\n\n    # Normalize impulse response\n    impulse_response = impulse_response / np.sum(impulse_response)\n\n    # Apply convolution\n    reverb_audio = signal.convolve(audio, impulse_response, mode='full')\n\n    # Trim to original length\n    reverb_audio = reverb_audio[:len(audio)]\n\n    return reverb_audio\n\n# Function to randomly collect background noises for each category\ndef collect_background_noises(chunks_dir, num_per_category=3):\n    \"\"\"Collect a set of random audio samples to use as background noise.\"\"\"\n    background_noises = {}\n    categories = [d for d in os.listdir(chunks_dir)\n                  if os.path.isdir(os.path.join(chunks_dir, d))]\n\n    for category in categories:\n        category_path = os.path.join(chunks_dir, category)\n        audio_files = [f for f in os.listdir(category_path) if f.endswith('.wav')]\n\n        # Select random files for background noise\n        if len(audio_files) > num_per_category:\n            selected_files = random.sample(audio_files, num_per_category)\n        else:\n            selected_files = audio_files\n\n        # Load audio files\n        noises = []\n        for file in selected_files:\n            audio_path = os.path.join(category_path, file)\n            try:\n                audio, sr = librosa.load(audio_path, sr=None)\n                noises.append((audio, sr))\n            except Exception as e:\n                print(f\"Error loading {audio_path}: {e}\")\n\n        background_noises[category] = noises\n\n    return background_noises\n\n# Function to apply random augmentations to an audio file\ndef augment_audio(audio, sr, category, background_noises):\n    \"\"\"Apply multiple random augmentations to an audio file.\"\"\"\n    # Randomly select the number of augmentations to apply (1-4)\n    num_augmentations = random.randint(1, 4)\n\n    # List of available augmentation methods\n    augmentation_methods = [\n        \"white_noise\", \"time_stretch\", \"pitch_shift\", \"volume_change\",\n        \"background_noise\", \"filter\", \"speed\", \"time_shift\", \"reverb\"\n    ]\n\n    # Randomly select which augmentations to apply\n    selected_augmentations = random.sample(augmentation_methods, num_augmentations)\n\n    # Apply the selected augmentations\n    augmented_audio = audio.copy()\n    augmentation_desc = []\n\n    for aug_method in selected_augmentations:\n        if aug_method == \"white_noise\":\n            # Add white noise\n            noise_factor = random.choice([0.003, 0.005, 0.01])\n            augmented_audio = add_white_noise(augmented_audio, noise_factor=noise_factor)\n            augmentation_desc.append(f\"noise_{noise_factor:.3f}\")\n\n        elif aug_method == \"time_stretch\":\n            # Time stretching\n            stretch_rate = random.choice([0.85, 0.9, 1.1, 1.15])\n            try:\n                augmented_audio = time_stretch(augmented_audio, rate=stretch_rate)\n                augmentation_desc.append(f\"stretch_{stretch_rate:.2f}\")\n            except:\n                # Skip if time stretching fails\n                pass\n\n        elif aug_method == \"pitch_shift\":\n            # Pitch shifting\n            n_steps = random.choice([-2, -1, 1, 2])\n            augmented_audio = pitch_shift(augmented_audio, sr=sr, n_steps=n_steps)\n            augmentation_desc.append(f\"pitch_{n_steps}\")\n\n        elif aug_method == \"volume_change\":\n            # Volume change\n            vol_factor = random.choice([0.75, 1.25, 1.5])\n            augmented_audio = change_volume(augmented_audio, factor=vol_factor)\n            augmentation_desc.append(f\"vol_{vol_factor:.2f}\")\n\n        elif aug_method == \"background_noise\":\n            # Background noise addition (from different category)\n            other_categories = [c for c in background_noises.keys() if c != category]\n            if other_categories:\n                # Select random category and noise\n                noise_category = random.choice(other_categories)\n                if background_noises[noise_category]:\n                    noise_audio, noise_sr = random.choice(background_noises[noise_category])\n\n                    # Apply background noise\n                    noise_factor = random.choice([0.1, 0.15, 0.2])\n                    augmented_audio = add_background_noise(augmented_audio, noise_audio, noise_factor=noise_factor)\n                    augmentation_desc.append(f\"bg_{noise_category}_{noise_factor:.2f}\")\n\n        elif aug_method == \"filter\":\n            # Apply filters\n            filter_type = random.choice(['lowpass', 'highpass'])\n            cutoff = random.choice([1000, 2000, 3000])\n            augmented_audio = apply_filter(augmented_audio, sr, filter_type=filter_type, cutoff=cutoff)\n            augmentation_desc.append(f\"filter_{filter_type[:2]}_{cutoff}\")\n\n        elif aug_method == \"speed\":\n            # Speed modification\n            speed_factor = random.choice([0.9, 1.1])\n            if speed_factor > 1.0:\n                augmented_audio = speed_up(augmented_audio, factor=speed_factor)\n                augmentation_desc.append(f\"speedup_{speed_factor:.2f}\")\n            else:\n                augmented_audio = slow_down(augmented_audio, factor=speed_factor)\n                augmentation_desc.append(f\"slowdown_{speed_factor:.2f}\")\n\n        elif aug_method == \"time_shift\":\n            # Time shifting\n            shift_factor = random.choice([0.1, 0.2, 0.3])\n            augmented_audio = time_shift(augmented_audio, shift_factor=shift_factor)\n            augmentation_desc.append(f\"shift_{shift_factor:.1f}\")\n\n        elif aug_method == \"reverb\":\n            # Reverb effect\n            decay = random.choice([1.0, 2.0, 3.0])\n            augmented_audio = reverb_effect(augmented_audio, sr, decay=decay)\n            augmentation_desc.append(f\"reverb_{decay:.1f}\")\n\n    # Create a description of all applied augmentations\n    aug_description = \"_\".join(augmentation_desc)\n\n    return augmented_audio, aug_description\n\n# Main function to augment the dataset to reach target file count per class\ndef augment_dataset_to_target(chunks_dir, target_files_per_class=1500):\n    \"\"\"Augment audio files in each category until reaching the target count.\"\"\"\n    print(f\"Starting audio augmentation to reach {target_files_per_class} files per class...\")\n\n    # Collect background noise samples\n    print(\"Collecting background noises...\")\n    background_noises = collect_background_noises(chunks_dir)\n\n    # Get all categories\n    categories = [d for d in os.listdir(chunks_dir) if os.path.isdir(os.path.join(chunks_dir, d))]\n\n    print(f\"Found {len(categories)} categories: {', '.join(categories)}\")\n\n    # Start timing\n    start_time = time.time()\n\n    # Process each category\n    for category in categories:\n        category_path = os.path.join(chunks_dir, category)\n\n        # Get existing files\n        existing_files = [f for f in os.listdir(category_path) if f.endswith('.wav')]\n        existing_count = len(existing_files)\n\n        print(f\"\\nCategory: {category}\")\n        print(f\"Existing files: {existing_count}\")\n\n        if existing_count >= target_files_per_class:\n            print(f\"Category {category} already has {existing_count} files. No augmentation needed.\")\n            continue\n\n        # Calculate how many more files we need\n        files_needed = target_files_per_class - existing_count\n        print(f\"Need to generate {files_needed} new files\")\n\n        # Track progress\n        with tqdm(total=files_needed, desc=f\"Augmenting {category}\") as pbar:\n            files_created = 0\n\n            while files_created < files_needed:\n                # Randomly select a file to augment\n                source_file = random.choice(existing_files)\n                source_path = os.path.join(category_path, source_file)\n\n                try:\n                    # Load audio file\n                    audio, sr = librosa.load(source_path, sr=None)\n\n                    # Apply random augmentations\n                    augmented_audio, aug_description = augment_audio(audio, sr, category, background_noises)\n\n                    # Create output filename\n                    base_name = os.path.splitext(source_file)[0]\n                    output_filename = f\"{base_name}_aug_{files_created+1}_{aug_description}.wav\"\n                    output_path = os.path.join(category_path, output_filename)\n\n                    # Save augmented audio\n                    sf.write(output_path, augmented_audio, sr)\n\n                    files_created += 1\n                    pbar.update(1)\n\n                except Exception as e:\n                    print(f\"Error processing {source_path}: {e}\")\n                    continue\n\n        # Verify final count\n        final_files = [f for f in os.listdir(category_path) if f.endswith('.wav')]\n        final_count = len(final_files)\n        print(f\"Final file count for {category}: {final_count}\")\n\n    # Report total time\n    total_time = time.time() - start_time\n    print(f\"\\nAugmentation complete! Total time: {total_time:.1f} seconds\")\n\n# Run the augmentation\nif __name__ == \"__main__\":\n    augment_dataset_to_target(chunks_dir, TARGET_FILES_PER_CLASS)\n    print(\"Audio dataset augmentation complete! Each class now has approximately 1500 files.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:53:07.631476Z","iopub.execute_input":"2025-05-09T02:53:07.632329Z","iopub.status.idle":"2025-05-09T03:03:13.805498Z","shell.execute_reply.started":"2025-05-09T02:53:07.632298Z","shell.execute_reply":"2025-05-09T03:03:13.804779Z"}},"outputs":[{"name":"stdout","text":"Starting audio augmentation to reach 1500 files per class...\nCollecting background noises...\nFound 7 categories: Metro_Station, University, Metro, Restaurant, Park, Shopping_Mall, Bus\n\nCategory: Metro_Station\nExisting files: 629\nNeed to generate 871 new files\n","output_type":"stream"},{"name":"stderr","text":"Augmenting Metro_Station: 100%|██████████| 871/871 [01:52<00:00,  7.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Final file count for Metro_Station: 1500\n\nCategory: University\nExisting files: 724\nNeed to generate 776 new files\n","output_type":"stream"},{"name":"stderr","text":"Augmenting University: 100%|██████████| 776/776 [01:26<00:00,  9.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Final file count for University: 1500\n\nCategory: Metro\nExisting files: 576\nNeed to generate 924 new files\n","output_type":"stream"},{"name":"stderr","text":"Augmenting Metro: 100%|██████████| 924/924 [01:47<00:00,  8.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Final file count for Metro: 1500\n\nCategory: Restaurant\nExisting files: 988\nNeed to generate 512 new files\n","output_type":"stream"},{"name":"stderr","text":"Augmenting Restaurant: 100%|██████████| 512/512 [01:01<00:00,  8.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Final file count for Restaurant: 1500\n\nCategory: Park\nExisting files: 496\nNeed to generate 1004 new files\n","output_type":"stream"},{"name":"stderr","text":"Augmenting Park: 100%|██████████| 1004/1004 [02:00<00:00,  8.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Final file count for Park: 1500\n\nCategory: Shopping_Mall\nExisting files: 690\nNeed to generate 810 new files\n","output_type":"stream"},{"name":"stderr","text":"Augmenting Shopping_Mall: 100%|██████████| 810/810 [01:35<00:00,  8.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Final file count for Shopping_Mall: 1500\n\nCategory: Bus\nExisting files: 1432\nNeed to generate 68 new files\n","output_type":"stream"},{"name":"stderr","text":"Augmenting Bus: 100%|██████████| 68/68 [00:09<00:00,  7.44it/s]","output_type":"stream"},{"name":"stdout","text":"Final file count for Bus: 1500\n\nAugmentation complete! Total time: 592.3 seconds\nAudio dataset augmentation complete! Each class now has approximately 1500 files.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchaudio\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.preprocessing import LabelBinarizer\nfrom transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Define paths - update these based on your Kaggle environment\nDATA_DIR = \"/kaggle/working/dataset\"  # Update this path\nOUTPUT_DIR = \"/kaggle/working/wav2vec_finetuned\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Create a function to load audio files and their labels\ndef load_dataset(data_dir):\n    data = []\n    \n    # Each folder in the data directory is a class\n    for class_name in os.listdir(data_dir):\n        class_dir = os.path.join(data_dir, class_name)\n        \n        if os.path.isdir(class_dir):\n            # Iterate through all audio files in the class directory\n            for file_name in os.listdir(class_dir):\n                if file_name.endswith(('.wav', '.mp3', '.flac')):  # Add other formats if needed\n                    file_path = os.path.join(class_dir, file_name)\n                    data.append((file_path, class_name))\n    \n    return data\n\n# Custom Dataset class for audio classification\nclass AudioClassificationDataset(Dataset):\n    def __init__(self, data, feature_extractor, max_length=16000*5):  # 5 seconds at 16kHz\n        self.data = data\n        self.feature_extractor = feature_extractor\n        self.max_length = max_length\n        \n        # Create mapping from class names to indices\n        self.class_names = sorted(list(set([label for _, label in data])))\n        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.class_names)}\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        audio_path, class_name = self.data[idx]\n        \n        # Load audio file\n        waveform, sample_rate = torchaudio.load(audio_path)\n        \n        # Convert to mono if stereo\n        if waveform.shape[0] > 1:\n            waveform = torch.mean(waveform, dim=0, keepdim=True)\n        \n        # Resample if necessary\n        if sample_rate != 16000:\n            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n            waveform = resampler(waveform)\n            sample_rate = 16000\n        \n        # Ensure the waveform is the right shape (1, n)\n        waveform = waveform.squeeze().numpy()\n        \n        # Truncate or pad to max_length\n        if len(waveform) > self.max_length:\n            waveform = waveform[:self.max_length]\n        else:\n            padding = np.zeros(self.max_length - len(waveform))\n            waveform = np.concatenate((waveform, padding))\n        \n        # Get features - more recent versions might not return attention_mask\n        inputs = self.feature_extractor(\n            waveform, \n            sampling_rate=16000, \n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=self.max_length\n        )\n        \n        # Create attention mask if not provided by feature extractor\n        if not hasattr(inputs, 'attention_mask'):\n            # Create attention mask based on non-zero values in input_values\n            # Assuming input_values are padded with zeros\n            attention_mask = (inputs.input_values != 0).float()\n        else:\n            attention_mask = inputs.attention_mask\n        \n        # Get label\n        label = self.class_to_idx[class_name]\n        \n        return {\n            \"input_values\": inputs.input_values.squeeze(),\n            \"attention_mask\": attention_mask.squeeze(),\n            \"labels\": torch.tensor(label)\n        }\n\ndef train_model(model, train_loader, val_loader, num_epochs, optimizer, scheduler, device):\n    train_losses = []\n    val_losses = []\n    train_accuracies = []\n    val_accuracies = []\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Initialize tqdm progress bar for training\n        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n        for batch in train_pbar:\n            input_values = batch[\"input_values\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device) if \"attention_mask\" in batch else None\n            labels = batch[\"labels\"].to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass with or without attention mask\n            if attention_mask is not None:\n                outputs = model(input_values=input_values, attention_mask=attention_mask, labels=labels)\n            else:\n                outputs = model(input_values=input_values, labels=labels)\n                \n            loss = outputs.loss\n            logits = outputs.logits\n            \n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            \n            running_loss += loss.item()\n            \n            # Calculate accuracy\n            _, predicted = torch.max(logits, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            # Update progress bar\n            current_loss = loss.item()\n            current_acc = (predicted == labels).sum().item() / labels.size(0)\n            train_pbar.set_postfix({'loss': f'{current_loss:.4f}', 'acc': f'{current_acc:.4f}'})\n            \n        epoch_loss = running_loss / len(train_loader)\n        epoch_acc = correct / total\n        train_losses.append(epoch_loss)\n        train_accuracies.append(epoch_acc)\n        \n        # Validation phase\n        model.eval()\n        val_running_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        # Initialize tqdm progress bar for validation\n        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Valid]\")\n        with torch.no_grad():\n            for batch in val_pbar:\n                input_values = batch[\"input_values\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device) if \"attention_mask\" in batch else None\n                labels = batch[\"labels\"].to(device)\n                \n                # Forward pass with or without attention mask\n                if attention_mask is not None:\n                    outputs = model(input_values=input_values, attention_mask=attention_mask, labels=labels)\n                else:\n                    outputs = model(input_values=input_values, labels=labels)\n                    \n                loss = outputs.loss\n                logits = outputs.logits\n                \n                val_running_loss += loss.item()\n                \n                # Calculate accuracy\n                _, predicted = torch.max(logits, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n                \n                # Update progress bar\n                current_loss = loss.item()\n                current_acc = (predicted == labels).sum().item() / labels.size(0)\n                val_pbar.set_postfix({'loss': f'{current_loss:.4f}', 'acc': f'{current_acc:.4f}'})\n        \n        val_epoch_loss = val_running_loss / len(val_loader)\n        val_epoch_acc = val_correct / val_total\n        val_losses.append(val_epoch_loss)\n        val_accuracies.append(val_epoch_acc)\n        \n        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, \"\n              f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n        \n        # Save memory\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    return train_losses, val_losses, train_accuracies, val_accuracies\n\ndef evaluate_model(model, test_loader, class_names, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    # Initialize tqdm progress bar for evaluation\n    test_pbar = tqdm(test_loader, desc=\"Evaluating\")\n    \n    with torch.no_grad():\n        for batch in test_pbar:\n            input_values = batch[\"input_values\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device) if \"attention_mask\" in batch else None\n            labels = batch[\"labels\"].to(device)\n            \n            # Forward pass with or without attention mask\n            if attention_mask is not None:\n                outputs = model(input_values=input_values, attention_mask=attention_mask)\n            else:\n                outputs = model(input_values=input_values)\n                \n            logits = outputs.logits\n            \n            # Get predictions\n            probs = torch.nn.functional.softmax(logits, dim=1)\n            _, preds = torch.max(logits, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n            \n            # Update progress bar with current batch accuracy\n            current_acc = (preds == labels).sum().item() / labels.size(0)\n            test_pbar.set_postfix({'acc': f'{current_acc:.4f}'})\n    \n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n    \n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    # Print classification report\n    report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0)\n    print(\"\\nClassification Report:\")\n    print(report)\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    cm = confusion_matrix(all_labels, all_preds)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'))\n    plt.close()\n    \n    # Calculate ROC curve and AUC for each class (one-vs-rest)\n    lb = LabelBinarizer()\n    lb.fit(range(len(class_names)))\n    all_labels_bin = lb.transform(all_labels)\n    all_probs = np.array(all_probs)\n    \n    plt.figure(figsize=(10, 8))\n    \n    for i, class_name in enumerate(class_names):\n        fpr, tpr, _ = roc_curve(all_labels_bin[:, i], all_probs[:, i])\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc:.2f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'roc_curve.png'))\n    plt.close()\n    \n    return accuracy, precision, recall, f1, all_preds, all_labels, all_probs\n\ndef plot_learning_curves(train_losses, val_losses, train_accs, val_accs):\n    # Plot training and validation loss\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    \n    # Plot training and validation accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label='Training Accuracy')\n    plt.plot(val_accs, label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'learning_curves.png'))\n    plt.close()\n\ndef main():\n    # Step 1: Load dataset\n    print(\"Loading dataset...\")\n    data = load_dataset(DATA_DIR)\n    \n    # Print dataset statistics\n    class_names = sorted(list(set([label for _, label in data])))\n    print(f\"Found {len(data)} audio files across {len(class_names)} classes:\")\n    for class_name in class_names:\n        count = sum(1 for _, label in data if label == class_name)\n        print(f\"  - {class_name}: {count} files\")\n    \n    # Step 2: Split into train, validation and test sets (80%, 10%, 10%)\n    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=[label for _, label in data])\n    train_data, val_data = train_test_split(train_data, test_size=0.125, random_state=42, stratify=[label for _, label in train_data])  # 0.125 of 80% = 10% of total\n    \n    print(f\"Train set: {len(train_data)} samples\")\n    print(f\"Validation set: {len(val_data)} samples\")\n    print(f\"Test set: {len(test_data)} samples\")\n    \n    # Step 3: Initialize model and feature extractor\n    print(\"Initializing model and feature extractor...\")\n    model_name = \"facebook/wav2vec2-base\"  # You can try other variants if needed\n    \n    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n    \n    # Create a model for sequence classification\n    model = Wav2Vec2ForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=len(class_names),\n        attention_dropout=0.1\n    )\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Step 4: Create datasets and dataloaders\n    print(\"Creating datasets and dataloaders...\")\n    \n    # Show progress with tqdm\n    print(\"Preparing train dataset...\")\n    train_dataset = AudioClassificationDataset(train_data, feature_extractor)\n    \n    print(\"Preparing validation dataset...\")\n    val_dataset = AudioClassificationDataset(val_data, feature_extractor)\n    \n    print(\"Preparing test dataset...\")\n    test_dataset = AudioClassificationDataset(test_data, feature_extractor)\n    \n    # Smaller batch size to avoid out of memory issues on Kaggle\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=8)\n    test_loader = DataLoader(test_dataset, batch_size=8)\n    \n    # Step 5: Set up training parameters\n    num_epochs = 20  # Start with a small number for testing\n    \n    # Optimizer with weight decay\n    optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n    \n    # Learning rate scheduler\n    total_steps = len(train_loader) * num_epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=total_steps\n    )\n    \n    # Step 6: Train the model\n    print(\"Training model...\")\n    train_losses, val_losses, train_accs, val_accs = train_model(\n        model, train_loader, val_loader, num_epochs, optimizer, scheduler, device\n    )\n    \n    # Step 7: Save the model\n    model.save_pretrained(OUTPUT_DIR)\n    feature_extractor.save_pretrained(OUTPUT_DIR)\n    print(f\"Model saved to {OUTPUT_DIR}\")\n    \n    # Step 8: Plot learning curves\n    print(\"Plotting learning curves...\")\n    plot_learning_curves(train_losses, val_losses, train_accs, val_accs)\n    \n    # Step 9: Evaluate on test set\n    print(\"Evaluating model on test set...\")\n    accuracy, precision, recall, f1, all_preds, all_labels, all_probs = evaluate_model(\n        model, test_loader, train_dataset.class_names, device\n    )\n    \n    # Save results to CSV\n    results_df = pd.DataFrame({\n        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n        'Value': [accuracy, precision, recall, f1]\n    })\n    results_df.to_csv(os.path.join(OUTPUT_DIR, 'metrics.csv'), index=False)\n    \n    print(\"\\nEvaluation complete. Results saved to\", OUTPUT_DIR)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:04:14.083199Z","iopub.execute_input":"2025-05-09T03:04:14.084183Z","execution_failed":"2025-05-09T03:05:39.234Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading dataset...\nFound 10500 audio files across 7 classes:\n  - Bus: 1500 files\n  - Metro: 1500 files\n  - Metro_Station: 1500 files\n  - Park: 1500 files\n  - Restaurant: 1500 files\n  - Shopping_Mall: 1500 files\n  - University: 1500 files\nTrain set: 7350 samples\nValidation set: 1050 samples\nTest set: 2100 samples\nInitializing model and feature extractor...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Creating datasets and dataloaders...\nPreparing train dataset...\nPreparing validation dataset...\nPreparing test dataset...\nTraining model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/20 [Train]:   0%|          | 0/919 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc451e4bcc34500a8bd485d5243819d"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}